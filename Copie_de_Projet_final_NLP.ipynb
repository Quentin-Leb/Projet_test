{
"nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkRL8EA_exYW"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # nécessaire pour éviter les erreurs de padding\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()\n",
        "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "examples = [\n",
        "\"What is the capital of France?\",\n",
        "\"What are the three primary colors?\",\n",
        "\"What does DNA stand for?\"\n",
        "]\n",
        "\n",
        "for instruction in examples:\n",
        "  prompt = f\"Instruction: {instruction}\\nRéponse:\"\n",
        "  output = generator(prompt, max_new_tokens=60)[0][\"generated_text\"]\n",
        "  print(\"\\n \", instruction)\n",
        "  print(output)"
      ],
      "metadata": {
        "id": "MZtT35Aoe5kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install datasets"
      ],
      "metadata": {
        "id": "LUqrmN77fVR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from datasets import load_dataset\n",
        "\n",
        "# Charger Alpaca, en gardant seulement 'instruction' et 'output'\n",
        "dataset = load_dataset(\"tatsu-lab/alpaca\")[\"train\"]\n",
        "dataset = dataset.remove_columns([\"input\"])\n",
        "\n",
        "# Optionnel : filtrer les réponses longues (> 80 mots)\n",
        "def is_simple(example):\n",
        "  return len(example[\"output\"].split()) <= 80\n",
        "\n",
        "dataset = dataset.filter(is_simple)"
      ],
      "metadata": {
        "id": "RdnqEw_TfeyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token # GPT-2 n'a pas de token de padding à la base"
      ],
      "metadata": {
        "id": "KF2LudZ7foyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(example):\n",
        "  prompt = f\"Instruction: {example['instruction']}\\nRéponse:\"\n",
        "  full_text = prompt + \" \" + example[\"output\"]\n",
        "  tokenized = tokenizer(full_text, truncation=True, max_length=128, padding=\"max_length\")\n",
        "  tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "  return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=dataset.column_names)"
      ],
      "metadata": {
        "id": "K4_wxLhPfxVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "base_model.resize_token_embeddings(len(tokenizer)) # Pour intégrer le token de padding"
      ],
      "metadata": {
        "id": "Y4OJD-FzgAKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "  r=8,\n",
        "  lora_alpha=16,\n",
        "  target_modules=[\"c_attn\"],\n",
        "  lora_dropout=0.1,\n",
        "  bias=\"none\",\n",
        "  task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)"
      ],
      "metadata": {
        "id": "z-M4eVIggE2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "output_dir = \"./gpt2-medium-alpaca-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=output_dir,\n",
        "  per_device_train_batch_size=8,\n",
        "  gradient_accumulation_steps=2,\n",
        "  logging_steps=1000,\n",
        "  learning_rate=2e-4,\n",
        "  num_train_epochs=1,\n",
        "  fp16=torch.cuda.is_available(),\n",
        "  save_strategy=\"epoch\",\n",
        "  save_total_limit=1,\n",
        "  report_to=[],\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "  model=model,\n",
        "  args=training_args,\n",
        "  train_dataset=tokenized_dataset,\n",
        "  data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "ScdWQ6JMggl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "metadata": {
        "id": "LLRoI_QUgybV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "instruction = \"What is the capital of France?\"\n",
        "prompt = f\"Instruction: {instruction}\\nRéponse:\"\n",
        "result = pipe(\n",
        "    prompt,\n",
        "    max_new_tokens=80,\n",
        "    temperature=0.7, # plus conservateur\n",
        "    top_p=0.9, # nucleus sampling\n",
        "    top_k=50, # coupe les options absurdes\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")[0][\"generated_text\"]\n",
        "\n",
        "print(\"\\n Réponse générée :\\n\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "RDxT_KOQg043"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "#  Instructions de test\n",
        "eval_instructions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Give three tips for staying healthy.\",\n",
        "    \"Create a list of five different animals\",\n",
        "    \"Who wrote 'Romeo and Juliet'?\"\n",
        "]\n",
        "\n",
        "#  Paramètres de génération (réglés pour du bon contrôle)\n",
        "gen_kwargs = dict(\n",
        "max_new_tokens=80,\n",
        "temperature=0.7,\n",
        "top_p=0.9,\n",
        "do_sample=True,\n",
        "eos_token_id=50256 # Fin de séquence pour GPT2\n",
        ")\n",
        "\n",
        "#  Charger modèle de base\n",
        "model_base = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "\n",
        "#  Charger modèle fine-tuné\n",
        "model_ft = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer_ft = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "model_ft.resize_token_embeddings(len(tokenizer_ft))\n",
        "model_ft = PeftModel.from_pretrained(model_ft, \"./gpt2-medium-alpaca-lora\")\n",
        "\n",
        "#  Pipelines\n",
        "pipe_base = pipeline(\"text-generation\", model=model_base, tokenizer=tokenizer_base, device_map=\"auto\")\n",
        "pipe_ft = pipeline(\"text-generation\", model=model_ft, tokenizer=tokenizer_ft, device_map=\"auto\")\n",
        "\n",
        "#  Comparaison\n",
        "for instr in eval_instructions:\n",
        "  prompt = f\"Instruction: {instr}\\nRéponse:\"\n",
        "\n",
        "  base_resp = pipe_base(prompt, **gen_kwargs)[0][\"generated_text\"]\n",
        "  ft_resp = pipe_ft(prompt, **gen_kwargs)[0][\"generated_text\"]\n",
        "\n",
        "  #  Affichage propre\n",
        "  print(\"\\n\" + \"=\"*80)\n",
        "  print(f\" Instruction: {instr}\")\n",
        "  print(\"\\n Réponse GPT2 (base):\")\n",
        "  print(base_resp.replace(prompt, \"\").strip())\n",
        "  print(\"\\n Réponse GPT2 fine-tuné:\")\n",
        "  print(ft_resp.replace(prompt, \"\").strip())\n",
        "  print(\"=\"*80)"
      ],
      "metadata": {
        "id": "VTKsa3BqhFbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "#  Charger modèle fine-tuné\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Charger les poids LoRA\n",
        "model = PeftModel.from_pretrained(base_model, \"./gpt2-medium-alpaca-lora\")\n",
        "\n",
        "#  Pipeline avec réglages pour qualité\n",
        "pipe = pipeline(\n",
        "  \"text-generation\",\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  device_map=\"auto\"\n",
        ")\n",
        "\n",
        "#  Paramètres de génération\n",
        "gen_kwargs = dict(\n",
        "  max_new_tokens=100,\n",
        "  temperature=0.7, # Gère la créativité (0.7 = bon équilibre)\n",
        "  top_p=0.9, # Nucleus sampling\n",
        "  do_sample=True,\n",
        "  eos_token_id=50256 # Pour forcer l’arrêt en fin de phrase\n",
        ")\n",
        "\n",
        "#  Interface utilisateur\n",
        "print(\" Assistant Fine-tuné Alpaca | Tape 'exit' pour quitter\\n\")\n",
        "\n",
        "while True:\n",
        "  instr = input(\" Instruction: \")\n",
        "  if instr.lower() in [\"exit\", \"quit\"]:\n",
        "    break\n",
        "\n",
        "  prompt = f\"Instruction: {instr}\\nRéponse:\"\n",
        "\n",
        "  try:\n",
        "      result = pipe(prompt, **gen_kwargs)[0][\"generated_text\"]\n",
        "      print(\"\\n Réponse générée :\")\n",
        "      print(result.replace(prompt, \"\").strip())\n",
        "      print(\"=\" * 80 + \"\\n\")\n",
        "  except Exception as e:\n",
        "      print(\" Erreur :\", e)"
      ],
      "metadata": {
        "id": "WUxqcOz6sCY_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
